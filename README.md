# ğŸ“Œ AI Mesai AsistanÄ± â€“ Backend

Bu proje, **FastAPI** tabanlÄ± bir yapay zekÃ¢ destekli backend uygulamasÄ±dÄ±r.  
AmaÃ§, Excel/CSVâ€™de tutulan **personel mesai verilerini** kullanarak doÄŸal dilde sorulan sorulara yanÄ±t vermektir.

Uygulama, **RAG (Retrieval-Augmented Generation)** mimarisi ile Ã§alÄ±ÅŸÄ±r:

1. KullanÄ±cÄ±nÄ±n sorusu embeddingâ€™e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r,  
2. Qdrant vektÃ¶r veritabanÄ± Ã¼zerinden en alakalÄ± veriler bulunur,  
3. Bu veriler LLaMA 3 modeline baÄŸlam olarak gÃ¶nderilir,  
4. Modelden alÄ±nan yanÄ±t kullanÄ±cÄ±ya dÃ¶ndÃ¼rÃ¼lÃ¼r.  

---

## ğŸš€ Ã–zellikler
- FastAPI tabanlÄ± REST API  
- TÃ¼rkÃ§e dil desteÄŸi  
- SentenceTransformer (all-MiniLM-L6-v2) ile embedding Ã§Ä±karma  
- Qdrant entegrasyonu ile vektÃ¶r arama  
- Ollama aracÄ±lÄ±ÄŸÄ±yla LLaMA 3 modeliyle yanÄ±t Ã¼retme  
- DoÄŸru ve veri temelli hesaplamalar yapma  

---

## ğŸ“‚ Dosya AÃ§Ä±klamasÄ±

### ğŸ”¹ `main.py`
Projenin ana backend dosyasÄ±dÄ±r.  

- `SoruModel`: APIâ€™ye gelen kullanÄ±cÄ± sorusunu temsil eden Pydantic modeli  
- `build_rag_prompt()`: KullanÄ±cÄ± sorusu + Qdrantâ€™tan gelen baÄŸlam ile LLaMA 3 iÃ§in Ã¶zel bir prompt oluÅŸturur  
- `/sor` endpointâ€™i:  
  - KullanÄ±cÄ±dan gelen soruyu iÅŸler  
  - Qdrant Ã¼zerinden baÄŸlam arar  
  - LLaMA 3 modelinden yanÄ±t Ã¼retir  

---

## âš™ï¸ Kurulum & Ã‡alÄ±ÅŸtÄ±rma
```bash
# Gerekli baÄŸÄ±mlÄ±lÄ±klarÄ±n yÃ¼klenmesi
pip install -r requirements.txt

# APIâ€™nin baÅŸlatÄ±lmasÄ±
uvicorn main:app --reload